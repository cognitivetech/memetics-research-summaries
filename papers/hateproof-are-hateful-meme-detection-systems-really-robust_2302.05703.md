# HateProof: Are Hateful Meme Detection Systems really Robust?

[HateProof: Are Hateful Meme Detection Systems really Robust?](https://arxiv.org/abs/2302.05703)

## Table of Contents
- [Abstract](#abstract)
- [1 Introduction](#1-introduction)
- [2 Related work](#2-related-work)
- [3 Datasets](#3-datasets)
- [4 Adversarial Attacks](#4-adversarial-attacks)
	- [4.2 Attacks on image](#42-attacks-on-image)
	- [5.2 Adversarial training with VILLA](#52-adversarial-training-with-villa)
- [6 Experimental Setup](#6-experimental-setup)
	- [6.1 Hate meme detection models](#61-hate-meme-detection-models)
- [7 Results](#7-results)
	- [7.2 Adversarial attacks](#72-adversarial-attacks)
- [8 Conclusion and Outlook](#8-conclusion-and-outlook)
- [A Ethical Consideration](#a-ethical-consideration)
	- [A.1 Unintentional bias](#a1-unintentional-bias)
	- [A.2 Code misuse](#a2-code-misuse)
	- [A.3 Carbon emission](#a3-carbon-emission)

## Abstract

**Study on Hateful Meme Detection Systems**

**Background:**
- Increase in hateful content on social media, especially memes
- Challenging to detect due to implicit payloads

**Vulnerabilities of Existing Systems:**
- Use case study to analyze vulnerabilities against external attacks
- Performance drop as high as 10% in macro-F1 score for certain attacks

**Findings:**
- Simple perturbations can make detection models highly vulnerable
- Empirical evidence from uni-modal and multi-modal settings

**Remedies:**
- Boost model's robustness using contrastive learning
- Adversarial training-based method - VILLA
- Ensemble of above approaches regains performance for certain attacks

**Implications:**
- First step towards addressing this crucial problem in an adversarial setting
- Inspires more investigations on improving hateful meme detection systems.

**Keywords:**
- Hateful memes, robustness, multi-modal, social media, ethics, accountability.

**CCS Concepts:**
- Computing methodologies
- Natural language processing
- Social and professional topics
- Censorship.


## 1 Introduction

**Social Media Hate Memes Detection: Challenges and Solutions**

**Background:**
- Misanthropists spread hatred using social media platforms through memes
- Manual screening of posts is challenging due to high volume of content dissemination
- Automatic moderation techniques required for effective hate speech detection

**Challenges:**
- Machine learning systems susceptible to unexpected outcomes
- Fusion mechanism in multimodal systems merges modalities and learns common representation
- Hateful memes comprise implicit properties, leading to potential bias
- Performance can be localized, posing significant challenges

**Misuse of Hate Meme Detection Systems:**
- Similar to deliberate misconduct in AI-based biometric authentication systems [57,62]
- Attackers have partial knowledge of the model and can integrate attacks into existing memes while maintaining message perception

**Proposed Approach:**
- Examine model vulnerability to partial model knowledge-based adversarial attacks
- Develop countermeasures using contrastive learning [10] and VILLA approach [17]
  - SimCLR: Function learns additional signals from simple augmentations of each data point in the training set
  - Adversarial training: Model trained on adversarial examples generated by perturbing one modality while keeping another unchanged

**Contributions:**
- Extend study to examine model vulnerability to partial model knowledge-based adversarial attacks and propose nine different attacks
- Develop countermeasures using contrastive learning [10] and VILLA approach [17] for handling adversarial inputs
- Propose an Ensemble of the above two approaches as the best solution for tackling various different forms of adversarial attacks

**Conclusion:**
- Analysis reveals potential loopholes that haters can adopt to escape detection systems
- Proactively propose generalized countermeasures efficient under many attack conditions.


## 2 Related work

**Hate Meme Detection: Research Topics and Techniques**

**Background:**
- Increasing popularity due to growth of hateful memes across social media platforms
- Multimodal pre-training followed by fine-tuning on smaller, supervised datasets
- Early and late fusion of features from each modality for final predictions

**Effective Systems:**
- Heavy reliance on large multimodal transformer models: VL-BERT, Oscar, Uniter, VisualBERT, LXMERT
- Ensemble of different visual-linguistic models for better detection performance

**Datasets:**
- Multiple datasets for diversity in detection task
- Other datasets focusing on sentiment, humor, offensiveness, and motive of memes
- Memes analyzed in various languages

**Adversarial Attacks:**
- Concern about model biasing and domain-specific responses
- Image tampering using image processing functions or GANs
- Color filter array (CFA) based spectroscopy to detect pixel level abnormalities
- Text-based attacks: adding LOVE to inputs, content code blurring, consider both modalities

**Model Robustness:**
- Previous work limited to uni-modal settings with end-to-end adversarial training (AT)
- Yang et al. emphasized robust fusion of modalities rather than end-to-end parameter training
- Injecting adversarial perturbations into feature space can improve model generalization on language understanding and visual-linguistic tasks
- Contrastive learning used to augment model robustness in text classification tasks
- Approaches insert visual-linguistic perturbation into embedding space of models to increase adversarial examples in input.


## 3 Datasets

**Datasets Used for Analysis**

**Three Benchmark Datasets:**
- **Fbhm [28]**: Contains 10,000 memes from Getty images with semi-artificial annotations
  - Five varieties: multimodal hate, unimodal hate, benign image and text confounders, random not-hateful examples
  - Annotated with hateful or non-hateful labels based on modalities
  - Split into training (85%), development (5%), and test (10%) sets
  - Developed by Kiela et al.

**HarMeme [46]**: Contains COVID-related memes from US social media
  - All original and shared across social media
  - Associated textual content obtained from Google Vision API
  - Resolution preserved
  - Divided into two categories: hateful (harmful and partially harmful) and non-hateful
  - Total of 3,544 data points; training (85%), development (5%), test (10%) sets

**Mami [16]**: Focuses on SUBTASK-A where memes are labeled as misogynistic or non-misogynistic
  - Fersini et al. developed it
  - Memes collected from social media websites with women as subjects or antifeminist content
  - Meme text extracted using Google Vision API
  - Balanced set of 10,000 instances; development (10%) and test (10%) sets randomly stratified.

**Analysis:**
- Proposed adversarial attacks applied only on the test set to analyze model robustness.


## 4 Adversarial Attacks

**Adversarial Attacks on Neural Networks**

**Susceptibility to Adversarial Attacks**:
- Neural networks are highly susceptible to minor changes (adversarial attacks) unless properly trained
- The human eye is almost insensitive to these minor changes, but they can hinder the prediction probabilities of the neural classifiers

**Types of Adversarial Attacks**:
- **Whitebox attacks**: Internal parameters and gradients of the models are known
- **Blackbox attacks**: Model is considered as an oracle, attacks are developed based on model confidence and output prediction probabilities

**Attacks in this Work**:
- Consider attacks based on partial knowledge of the models
- Involve nine different types of attacks, including:
  - **Simple attacks** such as SaltPepper noise
  - Textual attacks that directly affect the recognition quality of OCRs applied to memes

**Textual Attacks**:
- Unlike previous work, textual attacks are applied in a way that directly affects the recognition quality of OCRs on memes
- Studying the importance of text modality in real-time settings
- **Add**: Adding high polarity token (e.g., "LOVE" or "CARES") to the input text often misleads the classifier
- **Blur**: Blurring the text using OpenCV tool
- **SaltPepper-T**: Adding salt-and-pepper noise to only the text area of the image (by extracting the bounding box and adding white/black pixels)


### 4.2 Attacks on image

**Attack on Image:**
* **Unseen attack types**: Attacker cannot use gradient information to generate appropriate attacks, but can introduce random modifications to image while preserving message.
* **Image distortions using Gimp:**
  + SaltPepper-I: White and black pixels injected into the image in high and low settings, doesn't compromise perception or message.
  + Newsprint: Image poisoned with clustered dot dithering to simulate newspaper printing effect.
  + Spread: Swapping each pixel with another randomly chosen pixel for slightly jittery output, both high and low amounts used.
* **Combining text and image-based attacks:**
  + Common attack scenarios in each modality are combined (SaltPaper-I and Add token LOVE).

**Countermeasures:**
* **Contrastive learning through data augmentation:**
  + Data augmentation increases training data by introducing small mutations to original samples.
  + Contrastive learning represents a function learned on simple augmentations of each data point guided via loss function.
  + Integrates contrastive loss into any classifier to enhance generalization capabilities.
* **Algorithm for computing the contrastive loss function:**
  1- Learn representation by applying Augly to obtain synthetic augmentations.
  2- Define loss function based on pairwise cosine similarity between augmented versions of a single data point and spreading apart from different data points.
  3- Add the contrastive loss to the cross entropy loss used during training.
* **Countermeasures summary:** Data augmentation with synthetic augmentations and contrastive learning using the Augly library to increase training data and enhance model generalization capabilities.


### 5.2 Adversarial training with VILLA

**Adversarial Training for Hate Meme Detection:**
* Adversarial attacks on hate meme detection models have emerged as a critical problem [6,55]
* Robust classifier should identify adversarially poisoned images
* Training with augmented samples can increase robustness to some extent [19]
* Perturbation in the embedding space for image and text parts: BERT for text, regional embeddings for images [70]
* VILLA framework implemented to perturb one modality while keeping the other unchanged [17]
	+ Image-region features perturbed instead of pixels [55]
	+ Final embedding layer used for textual tokens
* Small perturbations keep instances within classification boundary and act as latent augmentation
* Uniform distribution based perturbation during pre-training and fine-tuning stages [70]
* Gaussian noise based perturbation results in better performance
* Table 2 shows macro-F1 scores for vanilla hate meme detection models.


## 6 Experimental Setup

**Hateful Memes Detection Experiment Setup**

1. Description of models: _______ (models for hateful meme detection)
2. Training framework: __________
3. Hyperparameters:
   - Learning rate: _________
   - Batch size: _________
   - Number of epochs: _________
   - Optimizer: _________
   - Loss function: _________
   - Evaluation metric: _________


### 6.1 Hate meme detection models

**Experimental Analysis of Hate Meme Detection Models**

**Model Selection:**
- Series of state-of-the-art hate meme detection models selected for testing vulnerability against adversarial attacks
- Models discussed in brief: VisualBERT, Uniter, Rob+Resnet

**VisualBERT:**
- Builds on BERT architecture and aligns input text with image regions using attention layer
- Token specific MLM used for pretraining
- Additional classification head with high learning rate and multi-sample dropout

**Uniter:**
- Early fusion approach, builds on text-image datasets for visual features and word piece encodings for textual ones
- Transformer based architecture to align modalities in shared embedding space
- Emphasizes fine-grained alignment with conditional masking and optimal transport

**Rob+Resnet:**
- Late fusion approach with separate extraction pipelines for image and text features
- Resnet used for image feature extraction, RoBERTa for text representation
- Concatenated features passed through feed-forward network with ReLu activation and dropout

**Countermeasures:**
- Three variants of VILLA: adversarial training during pretraining (VILLA-PT-UN), fine-tuning stage with uniform noise (VILLA-FT-UN), and fine-tuning stage with Gaussian noise (VILLA-FT-GN)
- Contrastive learning loss calculation for each modality
- Ensemble of countermeasures by averaging their prediction probabilities

**Model Training:**
- Performance evaluation before and after application of adversarial attacks
- Text preprocessing using Google OCR and BERT/RoBERTa tokenizers, image feature extraction using detectron2 framework with Faster R-CNN and ResNet-101
- Hyperparameters: variable for setting number of examples (1, 2, 3, 4, 5, 6, 10, 100), uniform or Gaussian noise type, temperature coefficient ùúèfor contrastive learning (0.5)

**Results:**
- Table 3: Percentage difference in macro-F1 hate meme detection systems with and without adversarial attacks
- Highlighted values indicate performance drop (RED) or improvement (GREEN) over baselines.


## 7 Results

**Results:**

1. **Vanilla Hate Meme Model Performance** (Macro-F1):
   - Table 2: Similar performance; Rob+Resnet for Mami, Uniter for others.
   - No significant differences between models (Mann-Whitney U test).


### 7.2 Adversarial attacks

**Vulnerability of Hate Meme Detection Models**
- **Table 3**: Illustrates vulnerability of existing state-of-the-art hate meme detection models using GoogleOCR text recognition model
- Each value represents percentage difference between macro F1 score before and after application of attacks
- Almost all models suffer drop in performance across different datasets and attack schemes:
  - **Mami Dataset**: Newsprint noise affects all models, with Rob+Resnet seeing highest drop of ~14.5%
  - **Spread-High** noise is next in line among unimodal attacks
  - **Multimodal** attack (Add+SaltPepper-I) adversely affects all models, with Rob+Resnet seeing highest drop of ~16.5%
  - **FBHM Dataset**: All models suffer less from attacks; VisualBERT model suffers most for Spread-High noise, Rob+Resnet for SaltPepper-I-High, and Uniter for Newsprint noise
  - For some textual noise, models show improvement in performance which may be attributed to training dataset already having similar adversarial examples
- **HarMeme Dataset**: All models suffer the most; Spread-High noise deteriorates performance of all models with 27.7%, 28.2%, and 25.4% reduction in macro-F1 scores for Uniter, VisualBERT, and Rob+Resnet respectively

**Countermeasures**
- Choose Uniter model since it showed slightly better performance in two out of three datasets
- Measure percentage difference (ùë•) in macro-F1 score between adverserially attacked model in absence and presence of countermeasures, and percentage difference (ùë¶) between unattacked model and attacked model with countermeasure
- Higher negative value of ùë• indicates more effective countermeasure in reducing attack's effect on the model
- Table 4 presents results of countermeasure approaches over Uniter model

**Key Observations**:
- For Fbhm and HarMeme datasets, Ensemble, VILLA-FT-GN, and CL-Ind-0.5 countermeasures are effective for multiple attack strategies
- On average, Ensemble countermeasure performs best followed by VILLA-FT-GN and CL-Ind-0.5 in that order
- For Fbhm dataset:
  - Highest improvement obtained for Add using Ensemble based countermeasure (6.39% improvement)
  - Large benefits seen in presence of Ensemble based countermeasure for Blur and SaltPepper-T attacks
  - VILLA-FT-GN and Ensemble most effective for SaltPepper-I-Low and SaltPepper-I-High attacks, respectively
  - Ensemble most effective for Spread-Low attack, VILLA-FT-GN most effective for Spread-High attack
- For HarMeme dataset:
  - CL-Ind-0.5 countermeasure best for Add attack
  - Ensemble based countermeasure works best for other text-based attacks
  - Ensemble most effective for SaltPepper-I-High and Spread-High attacks, VILLA-FT-GN most effective for Spread-Low attack
- None of the countermeasures work for certain attacks like Newsprint and Add+SaltPepper-I
- Countermeasures do not seem to work for Mami dataset due to very low resolution of original images (average bit depth 4.30 compared to 43.90 and 9.54 for HarMeme and Fbhm datasets)
- Impact of number of adversarial examples added during fine-tuning process:
  - Inclusion of up to five adversarial examples tolerable by the system
  - Performance steadily deteriorates after adding more than five adversarial examples


## 8 Conclusion and Outlook

**Study on Vulnerability of Multi-Modal Hate Meme Detection Systems**

**Key Findings:**
- Systematically analyzed vulnerability of widely popular visual linguistic models to partial model knowledge based adversaries
- Performance decreases significantly in response to text and image based attacks
- Image based attacks are more severe

**Countermeasures Proposed:**
- Contrusive learning method
- Adversarial training method
- Ensemble of these two methods works well for a large majority of attacks on two out of three datasets

**Future Directions:**
- Develop countermeasures for very severe adversarial attacks (e.g., Newsprint, Add+SaltPepper-I)
- Identify countermeasure approaches for low quality image datasets (e.g., Mami dataset)
- Explore more variants of attack schemes in future.

**Acknowledgments:**
- Supported by CATALPA - Center of Advanced Technology for Assisted Learning and Predictive Analytics at the Ferencsz University in Hagen, Germany.


## A Ethical Consideration

### A.1 Unintentional bias

**Statement on Biases in Meme Analysis Study:**

1. Unintentional biases present
2. No intention to harm groups or individuals
3. Determining harmful memes is subjective
4. Inevitable presence of biases in training and analysis


### A.2 Code misuse

* Publish code base for research
* Risk of misuse: generating attacks on social media algorithms
* Human moderation required alongside algorithmic detection for security


### A.3 Carbon emission

**Experiment Details:**

* Conducted experiments on GPUs for robustness and counter-measure modeling.
* Utilized a private infrastructure with a carbon efficiency of 0.432 kgCO2eq/kWh.
* Total computation: 300 hours on RTX 2080 Ti (TDP: 250W).
* Estimated emissions: 32.4 kgCO2eq, with no direct offsets.

---

Thank you to arXiv for use of its open access interoperability.